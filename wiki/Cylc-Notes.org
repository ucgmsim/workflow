#+TITLE: Cylc Notes

* Containers
** Creating a user is a bad idea
You have to bind-mount the cylc shared directory and work directory.
If you do this it bind-mounts as root in the container without read
permissions for any other user. So the scripts running as, for
example, a runner user cannot read/write to the output or shared file
directories. So you should run as =root= without any created users.
** Container run script
#+begin_src sh
  podman run --security-opt label=disable --mount type=bind,source=$CYLC_WORKFLOW_SHARE_DIR,dst=/share,rw=true --mount type=bind,source=.,dst=/out,rw=true -it runner nshm2022-to-realisation /nshmdb.db 0 /share/realisation.json
#+end_src
I think you only need =--security-opt label\=disable= if you use SELinux (like
on Fedora Silverblue). Not sure what that does but its effect is to
allow the bound =/share= and =/out= directories to be visible as =root
root= in the container rather than =ubuntu ubuntu= and be read/write
by no-one (including root!).

** Running in containers
To run in a container, you need to load the =Apptainer= module and then execute the script with =apptainer exec=.

In your =flow.cylc=:
#+begin_src conf
  [runtime]
      [[root]]
          platform = mahuika-slurm
          pre-script = """
          module load Apptainer
          """
#+end_src

The special =root= runtime specifies global configuration inherited by
all jobs. It is exactly equivalent to adding the =pre-script= and
=platform= variables to every job you define.

and then you need to have the something like the following in your =runtime= section.

#+begin_src conf
  [[nshm_to_realisation]]
      script = apptainer exec --bind "$CYLC_WORKFLOW_WORK_DIR:/out,$CYLC_WORKFLOW_SHARE_DIR:/share" ~/runner_latest.sif nshm2022-to-realisation /nshmdb.db 0 /share/realisation.json
#+end_src

Notice the =--bind= argument. This needs to be passed to every job
executing the container if you want to actually read the output of the
job (otherwise there is no communication with the host and the data is
destroyed at the end). For some reason it is not possible to set the
=APPTAINER_BIND= environment variable in the =environment= subsection
under =root=. This would allow us extract the bind directives and simply have

#+begin_src conf
  [runtime]
      [[root]]
          platform = mahuika-slurm
          pre-script = """
          module load Apptainer
          """
      [[nshm_to_realisation]]
          script = apptainer exec ~/runner_latest.sif nshm2022-to-realisation /nshmdb.db 0 /share/realisation.json
#+end_src

** Correctly setting the number of processes for multi-threaded applications (MAYBE?)
Some applications (Velocity Model generation, in particular) need to
be configured with the number of processes available to multithread
over. You can use the =nproc= utility to count the number of cores availible to the process. However, I think you need to be careful about precisely how you invoke the command. If you invoke it like
#+begin_src bash
  script = apptainer exec --bind "$CYLC_WORKFLOW_WORK_DIR:/out,$CYLC_WORKFLOW_SHARE_DIR:/share" /nesi/nobackup/nesi00213/containers/runner_latest.sif generate-velocity-model /share/realisation.json /share/Velocity_Model --num-threads $(nproc)
#+end_src
then I think cylc will evaluate =$(nproc)= at the login node (and so count way too few processors). Instead, you should invoke =$(nproc)= inside the container by escaping it with =sh=
#+begin_src bash
  script = apptainer exec --bind "$CYLC_WORKFLOW_WORK_DIR:/out,$CYLC_WORKFLOW_SHARE_DIR:/share" /nesi/nobackup/nesi00213/containers/runner_latest.sif sh -c 'generate-velocity-model /share/realisation.json /share/Velocity_Model --num-threads $(nproc)'
#+end_src
I am not actually sure if this is how it works, but I think it is and it cost nothing but an extra =sh= call to completely escape it. Maybe test this assumption?
** Containers should encapsulate scientific state
The container should contain /everything/, within reason. Default
parameter values, default velocity models, velocity model binaries,
etc. We want to minimise the places simulation state exists to
simplify validation and verification. See also [[Median event verification -> simulation]]
* Workflow
** Cylc workflows seem to lack ways of supplying initial inputs??
This is a problem for the initial input to the scripts (especially
in custom generation scenarios). The best solution I can think of is
moving files to =/share= after a =cylc install=.

** What is Rose?
There is this other tool that works well with cylc called [[https://metomi.github.io/rose/doc/html/index.html][Rose]]. This
appears to be a configuration suite that integrates with cylc to
provide input files and environment variables? Could be a used as an
alternative to writing our realisation files? Unclear of the scope of
the project or how to use it.

** Important job directories
- You share data between jobs in the =$CYLC_WORKFLOW_SHARE_DIR=.
- The current directory of every executing job is the output directory of the job.
- Each run of the workflow is kept in it's own run directory.
- The =runN= directory is symlinked to the last (or currently) running job executed for a workflow.
#+begin_example
  └── test_workflow
      ├── _cylc-install
      │   └── source -> /var/home/jake/cylc-src/test_workflow
      ├── run1
      │   ...
      ├── run2
      |   ...
      ├── run3
      │   ...
      ├── run4
      │   ...
      ├── run5
      │   ...
      ├── run6
      │   ├── flow.cylc # the workflow executed by cylc
      │   ├── log # logs and monitoring
      │   │   ├── config
      │   │   │   ├── 01-start-01.cylc    # not sure about this one
      │   │   │   └── flow-processed.cylc # or this one
      │   │   ├── db # work state database?
      │   │   ├── install
      │   │   │   └── 01-install.log # log to install the workflow (not generated by the job)
      │   │   ├── job
      │   │   │   └── 1
      │   │   │       ├── nshm_to_realisation
      │   │   │       │   ├── 01
      │   │   │       │   │   ├── job
      │   │   │       │   │   ├── job-activity.log
      │   │   │       │   │   ├── job.err  # stderr for the job
      │   │   │       │   │   ├── job.out  # stdout for the job
      │   │   │       │   │   └── job.status
      │   │   │       │   └── NN -> 01
      │   │   │       └── realisation_to_srf
      │   │   │           ├── 01
      │   │   │           │   ├── job
      │   │   │           │   ├── job-activity.log
      │   │   │           │   ├── job.err
      │   │   │           │   ├── job.out
      │   │   │           │   └── job.status
      │   │   │           └── NN -> 01
      │   │   └── scheduler
      │   │       ├── 01-start-01.log # not sure?
      │   │       └── log -> 01-start-01.log
      │   ├── share # CYLC_WORKFLOW_SHARE_DIR
      │   │   ├── realisation.json
      │   │   └── realisation.srf
      │   └── work # the output directory for each job that executes
      │       └── 1
      │           └── realisation_to_srf # output from the realisation to srf code (NB: this is $PWD of the realisation-to-srf job when executed)
      │               ├── gsf
      │               │   └── acton.gsf
      │               ├── rupture_0.srf
      │               └── srf
      │                   └── acton.srf
      └── runN -> run6
#+end_example

** Cylc needs files in a special directory
Cylc workflow files should live in the =~/cylc-src= directory. Workflows are installed to =~/cylc-run= with an invocation of =cylc install=.
** Cylc install installs more files
=cylc install= appears to copy all files present in the directory for the cylc workflow (in =~/cylc-src/workflow=) to the =~/cylc-run= directory. Is this a way of providing input files??
** Philosophically: what is a cylc workflow???
Is a cybershake run a number of independent cylc workflow generated from a template? Or, is it one cylc workflow that iterates over multiple faults. Not sure which of these two constitutes the happy path for cylc.

** The Cylc Install Process
According to the [[https://cylc.github.io/cylc-doc/stable/html/user-guide/installing-workflows.html#the-cylc-install-process][cylc install documentation]] inside the
=cylc-src/workflow= directory defining the workflow, directories and
files are copied to =cylc-run/workflow=. In addition
=cylc-run/workflow/bin= is in =PATH= for all jobs executed in the
workflow.

This means that an alternative to setting =APPTAINER_BIND= could simply be a file =workflow/bin/exec-in-container=

#+begin_src bash
  #!/usr/bin/env bash

  apptainer exec --bind "$CYLC_WORKFLOW_WORK_DIR:/out,$CYLC_WORKFLOW_SHARE_DIR:/share" $CONTAINER $0
#+end_src

and then a runtime like
#+begin_src conf
  [runtime]
      [[root]]
          platform = mahuika-slurm
          pre-script = """
          module load Apptainer
          """
          [[[environment]]]
              CONTAINER = ~/runner_latest.sif
      [[nshm_to_realisation]]
          script = exec-in-container nshm2022-to-realisation /nshmdb.db 0 /share/realisation.json
#+end_src

** Cylc + Jinja Templating: For large-scale and variable workflows?
Still unsure if cylc workflows should be one per realisation or one
workflow per cybershake run. Initially thought it was obviously one
per realisation, but now believe that cylc wants you to have multiple
different jobs run under the same workflow and then manage the
generation of jobs with jinja templating.

Here is the example they give for running multiple simulation for different cities:
#+begin_src conf
  #!Jinja2
  [meta]
      title = "Jinja2 city workflow example."
      description = """
          Illustrates use of variables and math expressions, and programmatic
          generation of groups of related dependencies and runtime properties.
      """

  [scheduler]
      allow implicit tasks = True

  {% set HOST = "SuperComputer" %}
  {% set CITIES = 'NewYork', 'Philadelphia', 'Newark', 'Houston', 'SantaFe', 'Chicago' %}
  {% set CITYJOBS = 'one', 'two', 'three', 'four' %}
  {% set LIMIT_MINS = 20 %}

  {% set CLEANUP = True %}

  [scheduling]
      initial cycle point = 2011-08-08T12
      [[graph]]
  {% if CLEANUP %}
          T23 = "clean"
  {% endif %}
          T00,T12 = """
              setup => get_lbc & get_ic # foo
      {% for CITY in CITIES %} {# comment #}
              get_lbc => {{ CITY }}_one
              get_ic => {{ CITY }}_two
              {{ CITY }}_one & {{ CITY }}_two => {{ CITY }}_three & {{ CITY }}_four
          {% if CLEANUP %}
              {{ CITY }}_three & {{ CITY }}_four => cleanup
      {% endif %}
  {% endfor %}
          """

  [runtime]
      [[on_{{ HOST }} ]]
          [[[remote]]]
              host = {{ HOST }}
              # (remote cylc directory is set in site/user config for this host)
          [[[directives]]]
              wall_clock_limit = "00:{{ LIMIT_MINS|int() + 2 }}:00,00:{{ LIMIT_MINS }}:00"

  {% for CITY in CITIES %}
      [[ {{ CITY }} ]]
          inherit = on_{{ HOST }}
      {% for JOB in CITYJOBS %}
      [[ {{ CITY }}_{{ JOB }} ]]
          inherit = {{ CITY }}
      {% endfor %}
  {% endfor %}
#+end_src

Note that they do not encourage creating one =flow.cylc= per city and build a template instead. This to me closely mirrors how we run cybershake over multiple faults.
Perhaps the best approach would be to use jinja to compile one huge =flow.cylc= for each cybershake run. Configuration via command line switches using jinja is also allowed. So a hypothetical =flow.cylc= file might be installed like

#+begin_src bash
  cylc install cybershake --gcmt --no-hf --no-bb --for-solutions $GCMT_SOLUTION_IDS # etc..
#+end_src

and then =cylc run cybershake/run1= would get start the cylc job running.

** Median event verification -> simulation workflow pipeline
Sung, Joel and I discussed how best to enable verification runs in Cybershake. The basic workflow would seem to be:

1. Simulate median events up to IM calculations and possible plotting. Enough information to verify the run.
2. Take *all* non-pertubated state: velocity models in particular and copy it over to the full Cybershake run.
3. The cybershake run will then execute *without generating any new data shared with the Median events*.

The reason to do this is to remove as many variables as possible
between verification and full runs. The software is the same (because
of containers), and all simulation data that is not being changed for
each realisation is also fixed between verification and full runs. If
instead we independently regenerate the velocity model (and other
data) for the full run, then we must account for the possibility that
a failed Cybershake run results from changes from the verification
run. This kinda defeats the purpose of a verification run.

** Cylc Does Plugins?

Reading the cylc documentation for some inspiration. It looks like we
can write plugins to cylc's main loop. I wonder if we could use this
to automatically record the core hour usage statistics for every job
that gets submitted? [[https://cylc.github.io/cylc-doc/stable/html/plugins/main-loop/index.html][See the main loop plugins docs]].

** Rose does archiving?
if we can figure out how to use rose we get a whole bunch of other
things for free. In particular, rose has support to archive workflows
(see [[https://cylc.github.io/cylc-doc/stable/html/workflow-design-guide/general-principles.html#confining-output-to-the-run-directory][here]], which refers to [[https://cylc.github.io/cylc-doc/stable/html/workflow-design-guide/general-principles.html#workflow-housekeeping][here]]).
* NESI

To properly use =cylc= on NESI you need to set the two environment variables =PROJECT= and =CYLC_VERSION=. The default cylc version is =7.x= which is very old, so set it to the latest.

#+begin_src bash
  export PROJECT=nesi00213
  export CYLC_VERSION=8.3.0 # latest at the time of writing
#+end_src
Add these to your =.bashrc=
* The Install Fault Step
The old workflow had an install fault step. It's job appeared to be to
generate coordinates for the stations and gridpoints for EMOD3D
simulations. It's written with a mix of old python and Rob's C code. I
think this can be done away with using some of the new qcore
geometry functions =qcore.grid= and =qcore.coordinates= in particular.

The files generated are gridpoint locations of stations loaded from a
plain text file listing the exact locations of the stations ={lon}
{lat} {station_name}=. Comments in this file have either =#= or =%= at
the start. This file can be read quickly with =pd.read_csv=.

In fact, all the files here can be read and written using
=pd.read_csv= and =pd.write_csv=.
** The statcord files
These files are plain text files recording the station locations in
EMOD3D gridpoint coordinates with each row having the format ={x:5d}
{y:5d} {depth?:5d} {station_name}=.
** The ll files
These files record latitude and longitude of the /grid points/ of the
stations. They have the format ={lat:11.5f} {lon:11.5f}
{station_name}=. Some stations are skipped if they inhabit the same
gridpoint.

